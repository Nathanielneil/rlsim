# DQN Configuration for UAV Navigation
algorithm: "DQN"
algorithm_params:
  # DQN specific parameters
  buffer_size: 100000
  learning_rate: 1.0e-4
  batch_size: 32
  gamma: 0.99
  target_update_freq: 1000
  
  # Exploration parameters
  exploration_fraction: 0.3
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.05
  
  # Training parameters
  total_timesteps: 1000000
  train_freq: 4
  gradient_steps: 1
  learning_starts: 10000
  
  # Network architecture
  net_arch: 
    - 512
    - 512
    - 256
  activation_fn: "relu"
  
  # Double DQN
  use_double_dqn: true
  
  # Dueling DQN
  use_dueling: true
  
  # Prioritized experience replay
  prioritized_replay: true
  prioritized_replay_alpha: 0.6
  prioritized_replay_beta0: 0.4
  prioritized_replay_beta_iters: 1000000

# Environment specific settings
env_config:
  action_type: "discrete"  # DQN uses discrete actions
  observation_type: "mixed"  # vision, state, mixed
  
  # Discrete action space (9 actions)
  discrete_actions:
    - [0, 0, 0, 0]      # hover
    - [2, 0, 0, 0]      # forward
    - [-2, 0, 0, 0]     # backward  
    - [0, 2, 0, 0]      # right
    - [0, -2, 0, 0]     # left
    - [0, 0, 1, 0]      # up
    - [0, 0, -1, 0]     # down
    - [0, 0, 0, 30]     # turn right
    - [0, 0, 0, -30]    # turn left

# Reward configuration
reward_weights:
  navigation: 1.0
  safety: 1.0
  efficiency: 0.5
  smoothness: 0.3
  collision_penalty: -10.0
  success_reward: 100.0
  step_penalty: -0.01
  
# Training settings
training:
  save_freq: 10000
  eval_freq: 5000
  eval_episodes: 10
  log_interval: 100
  
  # Checkpointing
  checkpoint_freq: 50000
  max_checkpoints: 5
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 100000
    min_delta: 0.01

# Model saving
model_save:
  save_format: "pytorch"
  compress: true
  include_replay_buffer: false