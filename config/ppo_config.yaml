# PPO Configuration for UAV Navigation
algorithm: "PPO"
algorithm_params:
  # PPO specific parameters
  clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  ppo_epochs: 4
  mini_batch_size: 64
  
  # Learning parameters
  learning_rate: 3.0e-4
  lr_schedule: "linear"  # linear, constant
  gamma: 0.99
  gae_lambda: 0.95
  
  # Training parameters
  total_timesteps: 1000000
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  
  # Network architecture
  policy_kwargs:
    net_arch: 
      - 256
      - 256
    activation_fn: "relu"
    
# Environment specific settings
env_config:
  action_type: "continuous"  # continuous or discrete
  observation_type: "mixed"  # vision, state, mixed
  
  # Action space (for continuous control)
  action_bounds:
    velocity_x: [-5.0, 5.0]
    velocity_y: [-5.0, 5.0] 
    velocity_z: [-2.0, 2.0]
    yaw_rate: [-90.0, 90.0]  # degrees per second

# Reward configuration  
reward_weights:
  navigation: 1.0
  safety: 1.0
  efficiency: 0.5
  smoothness: 0.3
  collision_penalty: -10.0
  success_reward: 100.0
  
# Training settings
training:
  save_freq: 10000  # Save model every N steps
  eval_freq: 5000   # Evaluate every N steps
  eval_episodes: 10
  log_interval: 100
  
  # Checkpointing
  checkpoint_freq: 50000
  max_checkpoints: 5
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 100000  # Steps without improvement
    min_delta: 0.01   # Minimum change to qualify as improvement
    
# Model saving
model_save:
  save_format: "pytorch"  # pytorch, onnx
  compress: true
  include_replay_buffer: false